#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Langchain å·¥å…·å®šä¹‰
åŒ…å«æ³•å¾‹æ£€ç´¢ã€å†…å®¹åˆ†æç­‰ä¸“ç”¨å·¥å…·
"""
import os
import sys
import faiss
import pickle
import numpy as np
from typing import List, Dict, Any, Optional
from langchain_core.tools import tool
from langchain_core.pydantic_v1 import BaseModel, Field
from sentence_transformers import SentenceTransformer
import warnings
warnings.filterwarnings("ignore")

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥ç°æœ‰æ¨¡å—
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


class LawRetrievalInput(BaseModel):
    """æ³•å¾‹æ£€ç´¢å·¥å…·è¾“å…¥æ¨¡å‹"""
    query: str = Field(description="æ³•å¾‹æŸ¥è¯¢é—®é¢˜")
    k: int = Field(default=5, description="è¿”å›ç»“æœæ•°é‡")
    min_score: float = Field(default=0.1, description="æœ€å°ç›¸ä¼¼åº¦åˆ†æ•°")


class LawRetriever:
    """æ³•å¾‹æ£€ç´¢å™¨ï¼ˆå¤ç”¨ç°æœ‰FAISSç´¢å¼•ï¼‰"""
    
    def __init__(self):
        # ä½¿ç”¨åŸºäºå½“å‰è„šæœ¬çš„ç»å¯¹è·¯å¾„
        script_dir = os.path.dirname(os.path.abspath(__file__))
        project_dir = os.path.dirname(os.path.dirname(script_dir))
        data_dir = os.path.join(project_dir, "Data")
        
        self.index_file = os.path.join(data_dir, "law_index.faiss")
        self.metadata_file = os.path.join(data_dir, "law_metadata.pkl")
        self.model_name = "BAAI/bge-large-zh-v1.5"
        self.index = None
        self.segments = None
        self.model = None
        self._initialized = False
    
    def initialize(self) -> bool:
        """åˆå§‹åŒ–æ£€ç´¢å™¨"""
        if self._initialized:
            return True
            
        try:
            # åŠ è½½æ¨¡å‹
            print("æ­£åœ¨åŠ è½½embeddingæ¨¡å‹...")
            try:
                self.model = SentenceTransformer(self.model_name)
                print(f"BGEä¸­æ–‡æ¨¡å‹åŠ è½½æˆåŠŸ: {self.model_name}")
            except Exception as e:
                print(f"ä¸»æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                backup_model = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
                self.model = SentenceTransformer(backup_model)
                print(f"å¤‡é€‰æ¨¡å‹åŠ è½½æˆåŠŸ: {backup_model}")
            
            # åŠ è½½FAISSç´¢å¼•
            if not os.path.exists(self.index_file):
                print(f"ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨: {self.index_file}")
                return False
            
            self.index = faiss.read_index(self.index_file)
            print(f"ç´¢å¼•åŠ è½½æˆåŠŸï¼åŒ…å« {self.index.ntotal} ä¸ªå‘é‡")
            
            # åŠ è½½å…ƒæ•°æ®
            if not os.path.exists(self.metadata_file):
                print(f"å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.metadata_file}")
                return False
                
            with open(self.metadata_file, 'rb') as f:
                self.segments = pickle.load(f)
            print(f"å…ƒæ•°æ®åŠ è½½æˆåŠŸï¼åŒ…å« {len(self.segments)} ä¸ªæ³•å¾‹æ¡æ¬¾")
            
            self._initialized = True
            return True
            
        except Exception as e:
            print(f"æ£€ç´¢å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def retrieve(self, query: str, k: int = 5, min_score: float = 0.1) -> List[Dict[str, Any]]:
        """æ£€ç´¢ç›¸å…³æ³•å¾‹æ¡æ¬¾"""
        if not self._initialized:
            if not self.initialize():
                return []
        
        try:
            # è·å–æŸ¥è¯¢å‘é‡
            query_embedding = self.model.encode(query, convert_to_numpy=True)
            query_embedding = query_embedding.astype(np.float32)
            
            # æ ‡å‡†åŒ–æŸ¥è¯¢å‘é‡
            query_embedding = query_embedding.reshape(1, -1)
            faiss.normalize_L2(query_embedding)
            
            # æœç´¢
            distances, indices = self.index.search(query_embedding, k)
            
            results = []
            for i, idx in enumerate(indices[0]):
                if idx != -1:  # æ£€æŸ¥æœ‰æ•ˆç´¢å¼•
                    segment = self.segments[idx].copy()
                    # IndexFlatIPè¿”å›çš„æ˜¯å†…ç§¯å€¼ï¼ˆç›¸ä¼¼åº¦ï¼‰ï¼Œç›´æ¥ä½¿ç”¨
                    similarity_score = float(distances[0][i])
                    segment['similarity_score'] = similarity_score
                    
                    # åº”ç”¨æœ€å°åˆ†æ•°è¿‡æ»¤
                    if similarity_score >= min_score:
                        results.append(segment)
            
            return results
            
        except Exception as e:
            print(f"æ£€ç´¢å¤±è´¥: {e}")
            return []


# å…¨å±€æ£€ç´¢å™¨å®ä¾‹
_law_retriever = None

def get_law_retriever():
    """è·å–æˆ–åˆ›å»ºæ£€ç´¢å™¨å®ä¾‹"""
    global _law_retriever
    if _law_retriever is None:
        _law_retriever = LawRetriever()
    return _law_retriever


def clean_query(query: str) -> str:
    """æ¸…ç†æŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œç§»é™¤å½±å“æ£€ç´¢çš„å…ƒç´ """
    import json
    import re
    
    cleaned = query.strip()
    
    # 1. æ£€æŸ¥æ˜¯å¦æ˜¯JSONæ ¼å¼çš„å‚æ•°å­—ç¬¦ä¸²ï¼ˆAgentæ¡†æ¶bugå¯¼è‡´ï¼‰
    if cleaned.startswith('{') and cleaned.endswith('}'):
        try:
            # å°è¯•è§£æJSONå¹¶æå–çœŸæ­£çš„æŸ¥è¯¢
            json_data = json.loads(cleaned)
            if 'query' in json_data:
                cleaned = json_data['query']
                print(f"ğŸ”§ æ£€æµ‹åˆ°JSONæ ¼å¼æŸ¥è¯¢ï¼Œå·²æå–: '{cleaned}'")
        except json.JSONDecodeError:
            # å¦‚æœJSONè§£æå¤±è´¥ï¼Œä¿æŒåŸæŸ¥è¯¢
            pass
    
    # 2. æ£€æŸ¥æ˜¯å¦æ˜¯ query="..." æ ¼å¼ï¼ˆReAct Agenté”™è¯¯æ ¼å¼ï¼‰
    query_pattern = r'query\s*=\s*["\']([^"\']+)["\']'
    match = re.search(query_pattern, cleaned)
    if match:
        cleaned = match.group(1)
        print(f"ğŸ”§ æ£€æµ‹åˆ°query=æ ¼å¼æŸ¥è¯¢ï¼Œå·²æå–: '{cleaned}'")
    
    # 3. æ£€æŸ¥æ˜¯å¦åŒ…å«å…¶ä»–å‚æ•°æ ¼å¼ï¼Œæå–queryéƒ¨åˆ†
    if 'query=' in cleaned and ('k=' in cleaned or 'min_score=' in cleaned):
        # å°è¯•æå–queryéƒ¨åˆ†
        parts = cleaned.split(',')
        for part in parts:
            if 'query=' in part:
                query_part = part.strip()
                # ç§»é™¤ query= å‰ç¼€å’Œå¼•å·
                query_part = re.sub(r'^.*query\s*=\s*["\']?', '', query_part)
                query_part = re.sub(r'["\'].*$', '', query_part)
                if query_part.strip():
                    cleaned = query_part.strip()
                    print(f"ğŸ”§ æ£€æµ‹åˆ°å¤åˆå‚æ•°æ ¼å¼ï¼Œå·²æå–query: '{cleaned}'")
                    break
    
    # 4. ç§»é™¤ç»“å°¾çš„é—®å·ï¼ˆä¿ç•™å¥å­ä¸­çš„é—®å·ï¼‰
    if cleaned.endswith('ï¼Ÿ') or cleaned.endswith('?'):
        cleaned = cleaned[:-1]
    
    # 5. å¦‚æœæŸ¥è¯¢å¤ªé•¿ï¼Œåªå–å‰é¢çš„æ ¸å¿ƒéƒ¨åˆ†
    if '\n' in cleaned:
        cleaned = cleaned.split('\n')[0].strip()
    
    return cleaned

@tool("law_retrieval", args_schema=LawRetrievalInput)
def law_retrieval_tool(query: str, k: int = 5, min_score: float = 0.1) -> str:
    """
    æ³•å¾‹æ¡æ–‡æ£€ç´¢å·¥å…·ã€‚
    
    ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦æœç´¢æ‰¾åˆ°ä¸æŸ¥è¯¢é—®é¢˜æœ€ç›¸å…³çš„æ³•å¾‹æ¡æ–‡ã€‚
    
    Args:
        query: æŸ¥è¯¢é—®é¢˜
        k: è¿”å›ç»“æœæ•°é‡ï¼ˆé»˜è®¤5ï¼‰
        min_score: æœ€å°ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆé»˜è®¤0.3ï¼‰
    
    Returns:
        æ ¼å¼åŒ–çš„æ³•å¾‹æ¡æ–‡ç»“æœ
    """
    # æ¸…ç†æŸ¥è¯¢ä»¥æé«˜æ£€ç´¢ç²¾åº¦
    cleaned_query = clean_query(query)
    retriever = get_law_retriever()
    results = retriever.retrieve(cleaned_query, k, min_score)
    
    if not results:
        return "æœªæ‰¾åˆ°ç›¸å…³çš„æ³•å¾‹æ¡æ–‡ã€‚"
    
    # æ ¼å¼åŒ–ç»“æœ
    formatted_results = []
    for i, result in enumerate(results, 1):
        article_info = f"ã€æ£€ç´¢ç»“æœ {i}ã€‘\n"
        article_info += f"æ³•å¾‹åç§°ï¼š{result['law_name']}\n"
        if result.get('chapter'):
            article_info += f"ç« èŠ‚ï¼š{result['chapter']}\n"
        if result.get('section'):
            article_info += f"èŠ‚ï¼š{result['section']}\n"
        article_info += f"æ¡æ–‡ï¼š{result['article']}\n"
        article_info += f"å†…å®¹ï¼š{result['content']}\n"
        article_info += f"ç›¸ä¼¼åº¦ï¼š{result['similarity_score']:.3f}\n"
        formatted_results.append(article_info)
    
    return "\n" + "\n".join(formatted_results)


class ContentAnalysisInput(BaseModel):
    """å†…å®¹åˆ†æå·¥å…·è¾“å…¥æ¨¡å‹"""
    content: str = Field(description="éœ€è¦åˆ†æçš„æ³•å¾‹å†…å®¹")
    analysis_type: str = Field(default="summary", description="åˆ†æç±»å‹ï¼šsummary, key_points, legal_advice")


@tool("content_analysis", args_schema=ContentAnalysisInput)
def content_analysis_tool(content: str, analysis_type: str = "summary") -> str:
    """
    æ³•å¾‹å†…å®¹åˆ†æå·¥å…·ã€‚
    
    å¯¹ç»™å®šçš„æ³•å¾‹å†…å®¹è¿›è¡Œåˆ†æï¼Œæå–å…³é”®ä¿¡æ¯ã€‚
    
    Args:
        content: éœ€è¦åˆ†æçš„å†…å®¹
        analysis_type: åˆ†æç±»å‹ï¼ˆsummary/key_points/legal_adviceï¼‰
    
    Returns:
        åˆ†æç»“æœ
    """
    if not content.strip():
        return "å†…å®¹ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œåˆ†æã€‚"
    
    # æ ¹æ®åˆ†æç±»å‹è¿”å›ä¸åŒçš„æ ¼å¼åŒ–æŒ‡å¼•
    if analysis_type == "summary":
        return f"ã€å†…å®¹æ€»ç»“ã€‘\néœ€è¦å¯¹ä»¥ä¸‹å†…å®¹è¿›è¡Œæ€»ç»“ï¼š\n{content[:500]}..."
    elif analysis_type == "key_points":
        return f"ã€å…³é”®è¦ç‚¹ã€‘\néœ€è¦æå–ä»¥ä¸‹å†…å®¹çš„å…³é”®æ³•å¾‹è¦ç‚¹ï¼š\n{content[:500]}..."
    elif analysis_type == "legal_advice":
        return f"ã€æ³•å¾‹å»ºè®®ã€‘\néœ€è¦åŸºäºä»¥ä¸‹å†…å®¹æä¾›æ³•å¾‹å»ºè®®ï¼š\n{content[:500]}..."
    else:
        return f"ã€å†…å®¹åˆ†æã€‘\néœ€è¦åˆ†æä»¥ä¸‹å†…å®¹ï¼š\n{content[:500]}..."


# å·¥å…·åˆ—è¡¨
ALL_TOOLS = [
    law_retrieval_tool,
    content_analysis_tool
]


def get_tools() -> List:
    """è·å–æ‰€æœ‰å¯ç”¨å·¥å…·"""
    return ALL_TOOLS


def initialize_tools() -> List:
    """åˆå§‹åŒ–æ‰€æœ‰å·¥å…·"""
    retriever = get_law_retriever()
    if retriever.initialize():
        return ALL_TOOLS
    else:
        return [] 